{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6032c83d-b24e-4901-8fc8-b9216f863a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with the file system\n",
    "from pathlib import Path\n",
    "\n",
    "# Loads, cleans and analyze data\n",
    "import pandas as pd\n",
    "\n",
    "# Do math operations with numbers and arrays\n",
    "import numpy as np\n",
    "\n",
    "# Create and customize graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Splits dataset into training and testing datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build random forest regressor model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Measures how accurate the model's (both linear regression and rfr) predictions are\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None) # Show all columns\n",
    "pd.set_option(\"display.max_rows\", None) # Show all rows\n",
    "\n",
    "# Display row into about 120 text characters before wrapping to a new line\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Raw dataset csv file name\n",
    "FILE_NAME = \"train.csv\"\n",
    "\n",
    "# Path to that file\n",
    "csv_path = f\"../data/raw/{FILE_NAME}\"\n",
    "\n",
    "# Load the file\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"Shape (rows, cols):\", df_raw.shape)\n",
    "\n",
    "# Print the first 10 rows of the dataset\n",
    "df_raw.head(10)\n",
    "\n",
    "# Check the data type of each column\n",
    "df_raw.info()\n",
    "\n",
    "# Display a graph that shows how house prices are spread\n",
    "# meaning, to see whether most houses are cheap, expensive or in between.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_raw[\"SalePrice\"].plot(kind=\"hist\", bins=40, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of House Prices\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Based on the graph above,\n",
    "# most of the bars are tall between 100k-200k.\n",
    "# meaning many houses cost around there.\n",
    "\n",
    "# The bars get shorter as prices go up. Fewer expensive houses.\n",
    "\n",
    "# Count how many missing (NaN) values each column has\n",
    "missing_counts = df_raw.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Show all columns with their missing values quantity\n",
    "print(\"Columns with missing values:\\n\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Some columns have NaN\n",
    "# This does not always mean the recorder forgot to record it in the dataset\n",
    "# It can also mean the value for the column does not exist\n",
    "\n",
    "# Meaning for example:\n",
    "# A house has no pool. So, Pool Quality is blank\n",
    "# This is not a mistake. That house just don't have that feature.\n",
    "# So, NaN means \"no pool\". Later, NaN will be replaced with \"NotAvailable\"\n",
    "\n",
    "# Another reason why some columns have NaN, \n",
    "# is because the recorder actually forgot to record it.\n",
    "# Maybe a house does have a garage, because the cell is empty\n",
    "# because the recorder forgot to record it\n",
    "\n",
    "# Replace \"doesn't exist\" (NaN) with \"NotAvailable\"\n",
    "\n",
    "notAvailable_columns = [\n",
    "    \"PoolQC\", \n",
    "    \"MiscFeature\", \n",
    "    \"Alley\", \n",
    "    \"Fence\", \n",
    "    \"FireplaceQu\",\n",
    "    \"GarageType\", \n",
    "    \"GarageFinish\", \n",
    "    \"GarageQual\", \n",
    "    \"GarageCond\",\n",
    "    \"BsmtQual\", \n",
    "    \"BsmtCond\", \n",
    "    \"BsmtExposure\", \n",
    "    \"BsmtFinType1\", \n",
    "    \"BsmtFinType2\",\n",
    "    \"MasVnrType\"\n",
    "]\n",
    "\n",
    "# For each column in that list, replace NaN with \"NotAvailable\"\n",
    "for curr_col in notAvailable_columns:    \n",
    "    df_raw[curr_col] = df_raw[curr_col].fillna(\"NotAvailable\")\n",
    "\n",
    "# Print how many missing values (NaN) remain in those columns\n",
    "print(\"\\nShow remaining missing values in these columns\")\n",
    "print(\"----------------------------------------------\\n\")\n",
    "print(df_raw[notAvailable_columns].isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# Print the first 10 rows of the dataset\n",
    "df_raw.head(10)\n",
    "\n",
    "# Replace NA values in these 3 numeric columns with their respective median value\n",
    "\n",
    "# List of numeric columns that have missing values\n",
    "empty_numeric_columns = [\"LotFrontage\", \"MasVnrArea\", \"GarageYrBlt\"]  \n",
    "\n",
    "# Go through each numeric column in that list\n",
    "for curr_column in empty_numeric_columns:           \n",
    "    if df_raw[curr_column].isna().any():          \n",
    "        # Replace NaN with the column's missing value\n",
    "        df_raw[curr_column] = df_raw[curr_column].fillna(df_raw[curr_column].median())\n",
    "\n",
    "\n",
    "# Print the first 10 rows of the dataset\n",
    "df_raw.head(10)\n",
    "\n",
    "# Fill the missing 'Electrical' with the most common value (Note: Only one cell is missing in Electrical column)\n",
    "\n",
    "# Find the most common value in the Electrical column\n",
    "most_common_elec = df_raw[\"Electrical\"].mode(dropna=True)[0]\n",
    "\n",
    "# Replace NaN with the most common value in the Electrical column (most_common_elec)\n",
    "df_raw[\"Electrical\"] = df_raw[\"Electrical\"].fillna(most_common_elec)\n",
    "\n",
    "# Calculate total duplicates in the dataset\n",
    "duplicates = df_raw.duplicated().sum()\n",
    "\n",
    "# Display duplicates\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Look for unusual (very large/small) values in NUMERIC columns\n",
    "# In other words, look for outliers\n",
    "df_raw.describe().T\n",
    "\n",
    "# Notes:\n",
    "# 25% (1st quartile) - meaning 25% of houses are cheaper than this value\n",
    "# 75% (3rd quartile) - meaning 75% of houses are cheaper than this value\n",
    "\n",
    "# For example, SalesPrice column:\n",
    "# 25% = 129,975  \n",
    "# 50% = 163,000  \n",
    "# 75% = 214,000\n",
    "\n",
    "# That means:\n",
    "# 25% of houses cost less than 129,975\n",
    "# 50% (the median) cost less than 163,000\n",
    "# 75% cost less than 214,000\n",
    "\n",
    "# how we decide what’s “too far”\n",
    "\n",
    "# We use a simple rule based on the interquartile range (IQR).\n",
    "# Q1 (25%) = price where 25% of houses are cheaper\n",
    "# Q3 (75%) = price where 75% of houses are cheaper\n",
    "# IQR = Q3 – Q1 (this is the “normal middle range”)\n",
    "\n",
    "# Then,\n",
    "# any value below Q1 − 1.5 × IQR\n",
    "# or above Q3 + 1.5 × IQR\n",
    "# is called an outlier\n",
    "\n",
    "# That’s just a rule of thumb, not a law — it helps us find numbers that are far outside the normal spread.\n",
    "\n",
    "# upper bound is a value, \n",
    "# and any house price above or equal to this value, \n",
    "# is considered as an outlier\n",
    "\n",
    "# For example:\n",
    "# Upper bound for SalesPrice = 340037.5\n",
    "# Any sale price value above this upper bound value, \n",
    "# is considered as an outlier\n",
    "# Examples (not from dataset, random numbers):\n",
    "# At row #400, the SalesPrice is $200,000 → ❌ not an outlier (normal range)\n",
    "# At row #56, the SalesPrice is $500,000 → ✅ outlier (too high)\n",
    "# At row #156, the SalesPrice is$700,000 → ✅ outlier (way too high)\n",
    "\n",
    "# Same logic applies to lower bound.\n",
    "# Any value below or equal to the lower bound value\n",
    "# is considered as an outlier\n",
    "\n",
    "# Get the names of all numeric columns in the dataset\n",
    "numeric_columns = df_raw.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "# This list will collect one summary per numeric column\n",
    "summaries = []\n",
    "\n",
    "for column_name in numeric_columns:\n",
    "    # Store the series (values) for this column\n",
    "    values = df_raw[column_name]\n",
    "\n",
    "    # Calculate 25% and 75% percentiles\n",
    "    q1 = values.quantile(0.25) # 25th percentile\n",
    "    q3 = values.quantile(0.75) # 75th percentile\n",
    "\n",
    "    # Calculate IQR (middle spread)\n",
    "    iqr = q3 - q1                            \n",
    "\n",
    "    # Set the lower and upper cutoffs for outliers using the 1.5xIQR rule\n",
    "    lower_bound = q1 - 1.5 * iqr # unusually low             \n",
    "    upper_bound = q3 + 1.5 * iqr # unusually high             \n",
    "\n",
    "    # Mark each value as an outlier (True) if it's below lower or above upper bound\n",
    "    is_outlier = (values < lower_bound) | (values > upper_bound)\n",
    "\n",
    "    # Count how many outliers the column has\n",
    "    outlier_count = int(is_outlier.sum())\n",
    "\n",
    "    # Calculate the percentage of total outliers of that column\n",
    "    outlier_percentage = round(100 * outlier_count / len(values), 2)\n",
    "\n",
    "    # Store a small summary dictionary for this column\n",
    "    summaries.append({\n",
    "        \"column\": column_name,\n",
    "        \"lower_bound\": round(lower_bound, 2),\n",
    "        \"upper_bound\": round(upper_bound, 2),\n",
    "        \"outlier_count\": outlier_count,\n",
    "        \"outlier_percentage\": outlier_percentage\n",
    "    })\n",
    "\n",
    "# Turn all summaries into a table, \n",
    "# sort by most outliers, \n",
    "# and reset the row index (meaning the rows in this table are renumbered, from 0, 1, 2, 3, and so on.)\n",
    "outlier_summary = pd.DataFrame(summaries).sort_values(\"outlier_count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Outlier summary (IQR rule) for all numeric columns:\\n\")\n",
    "display(outlier_summary)\n",
    "\n",
    "df_raw.head()\n",
    "\n",
    "# Save the cleaned dataset into the processed folder\n",
    "df_raw.to_csv('../data/processed/train_cleaned.csv', index=False)\n",
    "\n",
    "# Load the cleaned dataset and store it in df_cleaned\n",
    "df_cleaned = pd.read_csv('../data/processed/train_cleaned.csv')\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "# =================================== STEP 2: EDA (Starting) ===========================\n",
    "# **************************************************************************************\n",
    "\n",
    "# Checking correlation of all numeric columns with SalePrice\n",
    "# In other words,\n",
    "# check how strongly each one moves together with SalePrice.\n",
    "# It tells you how two numbers move together:\n",
    "# -- +1 (strong positive): when one goes up, the other also goes up (almost perfectly).\n",
    "# --  0 (no relation): they don’t move together in any consistent way.\n",
    "# -- −1 (strong negative): when one goes up, the other goes down (almost perfectly).\n",
    "\n",
    "# Store all numeric columns from the cleaned dataset\n",
    "numeric_columns = df_cleaned.select_dtypes(include='number')\n",
    "\n",
    "# Calculate correlation of each numeric column with SalePrice (Target variable)\n",
    "# In other words, find how strongly each numeric column is related to SalePrice\n",
    "correlation_with_target = numeric_columns.corr()['SalePrice'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation values, sorted from strongest to weakest\n",
    "print(\"Correlation of numeric columns with SalePrice:\\n\")\n",
    "print(correlation_with_target)\n",
    "\n",
    "# These are the numeric columns that greatly affect SalePrice column.\n",
    "# We will use these numeric columns. The rest will be ignored.\n",
    "\n",
    "# OverallQual      0.790982\n",
    "# GrLivArea        0.708624\n",
    "# GarageCars       0.640409\n",
    "# GarageArea       0.623431\n",
    "# TotalBsmtSF      0.613581\n",
    "# 1stFlrSF         0.605852\n",
    "# FullBath         0.560664\n",
    "# TotRmsAbvGrd     0.533723\n",
    "# YearBuilt        0.522897\n",
    "# YearRemodAdd     0.507101\n",
    "\n",
    "# Numerical columns selected, because they are strongly related to SalePrice (target)\n",
    "top_numericals = [\n",
    "    \"OverallQual\", \n",
    "    \"GrLivArea\", \n",
    "    \"GarageCars\", \n",
    "    \"GarageArea\",\n",
    "    \"TotalBsmtSF\", \n",
    "    \"1stFlrSF\", \n",
    "    \"FullBath\", \n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"YearBuilt\", \n",
    "    \"YearRemodAdd\"\n",
    "]\n",
    "\n",
    "# Create a tall figure to hold multiple subplots\n",
    "plt.figure(figsize=(12, 20))\n",
    "\n",
    "# Loop through each chosen numeric column and plot it against SalePrice\n",
    "for i, numerical in enumerate(top_numericals, 1):\n",
    "    plt.subplot(5, 2, i)  # 5 rows × 2 columns grid\n",
    "    plt.scatter(df_cleaned[numerical], df_cleaned[\"SalePrice\"], alpha=0.5)\n",
    "    plt.title(f\"{numerical} vs SalePrice\")\n",
    "    plt.xlabel(numerical)\n",
    "    plt.ylabel(\"SalePrice\")\n",
    "\n",
    "plt.tight_layout() # Reduce overlaps between subplots so labels fit nicely\n",
    "\n",
    "# Display all subplots\n",
    "plt.show()\n",
    "\n",
    "# Now, we’re just looking at categorical columns \n",
    "# (text columns like Neighborhood, HouseStyle, GarageType, etc.) \n",
    "# to see if different categories affect house price differently.\n",
    "\n",
    "# Store the names of all categorical columns\n",
    "category_columns = df_cleaned.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Display total category columns in the dataset\n",
    "print(\"Number of categorical columns:\", len(category_columns))\n",
    "print()\n",
    "\n",
    "# Dsiplay all category column names\n",
    "print(list(category_columns))\n",
    "\n",
    "# Check how each categorical column affects SalePrice\n",
    "# To do this, calculate the average SalePrice for each category value of a column\n",
    "\n",
    "# Meaning for example\n",
    "# In MSZoning column, it has values such as FV, RL, RH, RM and C\n",
    "# Take the SalePrice of every FV, and average them.\n",
    "# Do the same for RL, RH, RM and C\n",
    "# Repeat the steps above for all other category columns.\n",
    "\n",
    "# Loop through each categorical column\n",
    "for curr_column in category_columns:\n",
    "    # Calculate average SalePrice for each category in this column, \n",
    "    # then sorted from high to low, then round of to 2 decimal places\n",
    "    average_price = df_cleaned.groupby(curr_column)[\"SalePrice\"].mean().sort_values(ascending=False).round(2)\n",
    "    \n",
    "    # Count how many unique category values this column has\n",
    "    total_unique_value = df_cleaned[curr_column].nunique()\n",
    "\n",
    "    # Print column name and how many unique category value it has\n",
    "    print(f\"\\nColumn: {curr_column}  (unique categories: {total_unique_value})\")\n",
    "\n",
    "    # Print the average SalePrice for each unique category value, for every categorical column\n",
    "    print(average_price)\n",
    "\n",
    "# Based on the result above\n",
    "\n",
    "# Some columns (like Neighborhood, ExterQual, KitchenQual) \n",
    "# show huge differences between categories → they have a strong effect. \n",
    "\n",
    "# Others (like Utilities or PavedDrive) show small differences → weak or almost no effect.\n",
    "\n",
    "# For example:\n",
    "# Column: Neighborhood\n",
    "# NoRidge    335295.32\n",
    "# NridgHt    316270.62\n",
    "# StoneBr    310499.00\n",
    "# ...\n",
    "# MeadowV     98576.47\n",
    "\n",
    "# The lowest average price is about 98k, and the highest is 335k.\n",
    "# That’s more than a 3× difference.\n",
    "# That means houses in some neighborhoods sell for much higher prices than others.\n",
    "# So, Neighborhood strongly affects SalePrice.\n",
    "\n",
    "# Another column:\n",
    "# Column: Utilities\n",
    "# AllPub    180950.96\n",
    "# NoSeWa    137500.00\n",
    "\n",
    "# Only 2 categories, and the price difference is about 43 k.\n",
    "# That’s not a huge jump compared to the average house price (≈ 180 k),\n",
    "# and also, almost every house has “AllPub” utilities. very few have “NoSeWa”.\n",
    "# So, Utilities barely affects price. It’s almost the same everywhere.\n",
    "\n",
    "# Now we need to select which category column greatly affect the target variable\n",
    "# instead of manually going through each 43 columns and select the columns that are needed.\n",
    "\n",
    "# Calculate how strongly each categorical column affects SalePrice\n",
    "\n",
    "# A dictionary to store how much each categorical column affects sale price\n",
    "influence_scores = {}\n",
    "\n",
    "# For every categorical column, compare average prices across its unique category value\n",
    "# only compare higest and lowest average price value.\n",
    "for curr_column in category_columns:\n",
    "\n",
    "    # Group rows by the categories in this column and compute the average SalePrice for each category\n",
    "    # In other words, it stores the average price of each unique category value in that category column\n",
    "    average_price = df_cleaned.groupby(curr_column)[\"SalePrice\"].mean()\n",
    "\n",
    "    # Skip columns with only 1 unique category value\n",
    "    if len(average_price) > 1:  \n",
    "\n",
    "        # Difference between the highest and lowest average price across categories\n",
    "        gap = average_price.max() - average_price.min()\n",
    "\n",
    "        # Key is category name, value is the price difference\n",
    "        influence_scores[curr_column] = gap\n",
    "\n",
    "# Turn the results into a table and sort from expensive to cheapest\n",
    "influence_df = (\n",
    "    pd.DataFrame(list(influence_scores.items()), columns=[\"Column\", \"PriceDifference\"])\n",
    "    .sort_values(by=\"PriceDifference\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Round to 2 decimals for readability\n",
    "influence_df[\"PriceDifference\"] = influence_df[\"PriceDifference\"].round(2)\n",
    "\n",
    "# Show the table of categorical columns and their price gaps\n",
    "print(influence_df)\n",
    "\n",
    "# Select top 10 (first 10 since it is already sorted)\n",
    "top_influence = influence_df.head(10)\n",
    "\n",
    "print(\"Top 10 most influential categorical columns:\")\n",
    "print(top_influence)\n",
    "\n",
    "# Loop through each categorical column in the selected top_influence list (column name)\n",
    "for col in top_influence[\"Column\"]:\n",
    "\n",
    "    # Get the average SalePrice for each unique category value, then sort them from highest to lowest\n",
    "    average = df_cleaned.groupby(col)[\"SalePrice\"].mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(average.index, average.values)\n",
    "    plt.title(f\"Average SalePrice by {col}\")\n",
    "    plt.ylabel(\"Mean Sale Price ($)\")\n",
    "    plt.xlabel(col)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# These 10 categorical columns are selected because they greatly influence the sale price\n",
    "top_category = [\n",
    "    \"PoolQC\", \n",
    "    \"ExterQual\", \n",
    "    \"RoofMatl\", \n",
    "    \"Neighborhood\", \n",
    "    \"Condition2\", \n",
    "    \"KitchenQual\", \n",
    "    \"Exterior2nd\", \n",
    "    \"BsmtQual\", \n",
    "    \"FireplaceQu\", \n",
    "    \"Exterior1st\"\n",
    "]\n",
    "\n",
    "# ========================================= STEP 3 - FEATURE ENGINEERING (Start) =========================\n",
    "# ***********************************************************************************************\n",
    "\n",
    "# These are the columns the models will work on:\n",
    "\n",
    "# Numerical columns:\n",
    "# top_numericals = [\n",
    "#     \"OverallQual\", \n",
    "#     \"GrLivArea\", \n",
    "#     \"GarageCars\", \n",
    "#     \"GarageArea\",\n",
    "#     \"TotalBsmtSF\", \n",
    "#     \"1stFlrSF\", \n",
    "#     \"FullBath\", \n",
    "#     \"TotRmsAbvGrd\",\n",
    "#     \"YearBuilt\", \n",
    "#     \"YearRemodAdd\"\n",
    "# ]\n",
    "\n",
    "# Categorical columns:\n",
    "# top_category = [\n",
    "#     \"PoolQC\", \n",
    "#     \"ExterQual\", \n",
    "#     \"RoofMatl\", \n",
    "#     \"Neighborhood\", \n",
    "#     \"Condition2\", \n",
    "#     \"KitchenQual\", \n",
    "#     \"Exterior2nd\", \n",
    "#     \"BsmtQual\", \n",
    "#     \"FireplaceQu\", \n",
    "#     \"Exterior1st\"\n",
    "# ]\n",
    "\n",
    "# Create a copy data frame\n",
    "# but this copy will only have the selected 20 columns + 1 (target)\n",
    "\n",
    "# Make a full copy of the cleaned dataset to avoid changing the original\n",
    "df_selected = df_cleaned.copy()\n",
    "\n",
    "# Combine the top numeric, top categorical, and target columns into one list\n",
    "selected_columns = top_numericals + top_category + [\"SalePrice\"]\n",
    "\n",
    "# Keep only those selected columns in the new dataframe\n",
    "df_selected = df_selected[selected_columns]\n",
    "\n",
    "print(f\"Shape: {df_selected.shape}\")\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "df_selected.head()\n",
    "\n",
    "# Save this df_model as a csv file, and store it accordingly\n",
    "df_selected.to_csv('../data/processed/train_selected_cols.csv', index=False)\n",
    "\n",
    "# Encode categorical columns\n",
    "# because ml models can't work with text data. only numbers.\n",
    "# so, we turn (encode) each categorical value from text to number\n",
    "\n",
    "# Before we encode, \n",
    "# we need to identify which categorical column\n",
    "# are ordinal or nominal.\n",
    "\n",
    "# Ordinal\n",
    "# Categories that clear order or ranking (example: Excellent -> Good -> Average -> Fair -> Poor)\n",
    "# From the top_category variable,\n",
    "# There are 5 ordinal categories:\n",
    "# PoolQC, ExterQual, KitchenQual, BsmtQual, FireplaceQu\n",
    "# They describe quality levels\n",
    "\n",
    "# Nominal\n",
    "# Categories with no particular order or ranking\n",
    "# From the top_category variable,\n",
    "# There are 5 nominal categories:\n",
    "# Neighborhood, RoofMatl, Condition2, Exterior1st, Exterior2nd\n",
    "\n",
    "# top_category = [\n",
    "#     \"PoolQC\",        ---> Ordinal\n",
    "#     \"ExterQual\",     ---> Ordinal\n",
    "#     \"RoofMatl\",      ---> Nominal\n",
    "#     \"Neighborhood\",  ---> Nominal\n",
    "#     \"Condition2\",    ---> Nominal\n",
    "#     \"KitchenQual\",   ---> Ordinal\n",
    "#     \"Exterior2nd\",   ---> Nominal\n",
    "#     \"BsmtQual\",      ---> Ordinal\n",
    "#     \"FireplaceQu\",   ---> Ordinal\n",
    "#     \"Exterior1st\"    ---> Nominal\n",
    "# ]\n",
    "\n",
    "# Ordinal category\n",
    "\n",
    "# Define the order (worst -> best) for each ordinal categorical column\n",
    "ordinal_orders = {\n",
    "    \"PoolQC\":      [\"NotAvailable\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"ExterQual\":   [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"KitchenQual\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtQual\":    [\"NotAvailable\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"FireplaceQu\": [\"NotAvailable\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "}\n",
    "\n",
    "# Convert each ordered category column into numeric codes (0 = lowest, larger = better\n",
    "for col, order in ordinal_orders.items():\n",
    "    df_selected[col] = pd.Categorical(df_selected[col], categories=order, ordered=True).codes\n",
    "\n",
    "# print(df_selected[list(ordinal_orders.keys())].head())\n",
    "for col in ordinal_orders:\n",
    "    print(f\"{col} unique codes:\", sorted(df_selected[col].unique()))\n",
    "\n",
    "# Notes about cell above:\n",
    "    # \"PoolQC\":      \n",
    "    # [\"NotAvailable\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    # [\"NotAvailable\", \"Fair\", \"Average\", \"Good\", \"Excellent\"] \n",
    "\n",
    "    # \"ExterQual\":\n",
    "    # [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    # [\"Poor\", \"Fair\", \"Average\", \"Good\", \"Excellent]\n",
    "\n",
    "    # \"KitchenQual\": \n",
    "    # [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    # [\"Poor\", \"Fair\", \"Average\", \"Good\", \"Excellent]\n",
    "\n",
    "    # \"BsmtQual\":    \n",
    "    # [\"NotAvailable\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    # [\"NotAvailable\", \"Fair\", \"Average\", \"Good\", \"Excellent\"] \n",
    "    \n",
    "    # \"FireplaceQu\": \n",
    "    # [\"NotAvailable\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    # [\"NotAvailable\", \"Poor\", \"Fair\", \"Average\", \"Good\", \"Excellent\"]\n",
    "\n",
    "# Important note:\n",
    "# If you look at the dataset, or the result below,\n",
    "# you'll find out, for example, PoolQC does not have TA\n",
    "\n",
    "df_selected.head()\n",
    "\n",
    "# Nominal (no-order) categorical columns\n",
    "nominal_cols = [\"RoofMatl\", \"Neighborhood\", \"Condition2\", \"Exterior2nd\", \"Exterior1st\"]\n",
    "\n",
    "# One-hot encode them, store result in df_encoded\n",
    "df_encoded = pd.get_dummies(df_selected, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "print(\"Before encoding nominal categorical columns:\", df_selected.shape)\n",
    "print(\"After encoding nominal categorical columns:\", df_encoded.shape)\n",
    "df_encoded.head()\n",
    "\n",
    "# ========================================= STEP 4 - ML MODELING (Start) =========================\n",
    "# ***********************************************************************************************\n",
    "\n",
    "# Target variable\n",
    "target = \"SalePrice\"\n",
    "\n",
    "# Separate X (independent variables) and y (dependent variable)\n",
    "\n",
    "# Store all columns except the target (features used to predict)\n",
    "X = df_encoded.drop(columns=[target])\n",
    "\n",
    "# Store the target variable only (the thing to predict)\n",
    "y = df_encoded[target]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "# into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# X_train --> Features used to teach the model\n",
    "# y_train --> The real answers (prices) for those same training rows\n",
    "\n",
    "# X_test --> Features the model has not seen\n",
    "# y_test --> The real prices for the test rows\n",
    "\n",
    "# X_train, y_train --> used to train the model (it has the target column)\n",
    "# X_test, y_test --> used to test the model (it has no target column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Testing set:\", X_test.shape)\n",
    "\n",
    "# LINEAR REGRESSION\n",
    "\n",
    "# Create and train the model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred_lin is the prices predicted by the model\n",
    "# y_test is the actual real house prices from the dataset\n",
    "\n",
    "# The trained model looks at the test inputs (X_test),\n",
    "# and guesses the prices. \n",
    "# Those guesses are stored in y_pred_linear_regression\n",
    "y_pred_linear_regression  = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate error metrics\n",
    "# It measures how far off the guesses are\n",
    "rmse = root_mean_squared_error(y_test, y_pred_linear_regression)\n",
    "mae  = mean_absolute_error(y_test, y_pred_linear_regression)\n",
    "r2   = r2_score(y_test, y_pred_linear_regression)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Linear Regression — RMSE: ${rmse:,.0f}\")\n",
    "print(f\"Linear Regression —  MAE: ${mae:,.0f}\")\n",
    "print(f\"Linear Regression —   R²:  {r2:.3f}\")\n",
    "\n",
    "# What the results mean\n",
    "\n",
    "# MAE - $23,244\n",
    "# on average, the price guesses are off by about $23,000 per house. (Lower is better.)\n",
    "\n",
    "# RMSE - $48, 193\n",
    "# when we penalize big mistakes more, \n",
    "# the “typical” miss is about $48k. (Means you likely have some big errors/outliers.)\n",
    "\n",
    "# R2 - 0.697\n",
    "# The model explains about 70% of why prices go up/down (decent, not perfect)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_linear_regression, alpha=0.6, edgecolors=\"k\")\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Actual vs Predicted Sale Prices (Linear Regression)\")\n",
    "\n",
    "# Draw a perfect prediction line\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2, label=\"Perfect Prediction\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare actual and predicted prices\n",
    "comparison = pd.DataFrame({\n",
    "    \"Actual_Price\": y_test.values,\n",
    "    \"Predicted_Price\": y_pred_linear_regression\n",
    "})\n",
    "\n",
    "# Round for readability\n",
    "comparison[\"Actual_Price\"] = comparison[\"Actual_Price\"].round(2)\n",
    "comparison[\"Predicted_Price\"] = comparison[\"Predicted_Price\"].round(2)\n",
    "\n",
    "# Show the first 20 rows\n",
    "print(comparison.head(20))\n",
    "\n",
    "# Create a small sample (like 20 houses)\n",
    "sample = comparison.head(20)\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sample.index, sample[\"Actual_Price\"], label=\"Actual Price\", marker=\"o\")\n",
    "plt.plot(sample.index, sample[\"Predicted_Price\"], label=\"Predicted Price\", marker=\"x\")\n",
    "plt.title(\"Actual vs Predicted Sale Prices (Sample)\")\n",
    "plt.xlabel(\"House Index\")\n",
    "plt.ylabel(\"Sale Price ($)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# RANDOM FOREST REGRESSOR\n",
    "\n",
    "# Create the random forest regressor model\n",
    "random_forest_regressor_model = RandomForestRegressor(\n",
    "    n_estimators=300, # number of trees in the forest\n",
    "    random_state=42, # reproducible results\n",
    "    n_jobs=-1 # use all CPU cores (faster training)\n",
    ")\n",
    "# What above means:\n",
    "# Create a forest of 300 trees\n",
    "# Each tree learns slightly different patterns\n",
    "# The final prediction is the average of all trees' predictions\n",
    "\n",
    "# Train the model with training data\n",
    "random_forest_regressor_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict prices using the test set\n",
    "y_pred_random_forest_regressor = random_forest_regressor_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_random_forest_regressor))\n",
    "rf_mae  = mean_absolute_error(y_test, y_pred_random_forest_regressor)\n",
    "rf_r2   = r2_score(y_test, y_pred_random_forest_regressor)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Random Forest — RMSE: ${rf_rmse:,.0f}\")\n",
    "print(f\"Random Forest —  MAE: ${rf_mae:,.0f}\")\n",
    "print(f\"Random Forest —   R²:  {rf_r2:.3f}\")\n",
    "\n",
    "# For comparison\n",
    "\n",
    "# Linear Regression:\n",
    "# RMSE: $48,193\n",
    "# MAE: $23,244\n",
    "# R²:  0.697\n",
    "\n",
    "# Random Forest Regressor\n",
    "# RMSE: $31,642\n",
    "# MAE: $18,563\n",
    "# R²:  0.869\n",
    "\n",
    "# Compare actual and predicted prices\n",
    "rf_comparison = pd.DataFrame({\n",
    "    \"Actual_Price\": y_test.values,\n",
    "    \"Predicted_Price_RF\": y_pred_random_forest_regressor\n",
    "})\n",
    "\n",
    "# Round for readability\n",
    "rf_comparison[\"Actual_Price\"] = rf_comparison[\"Actual_Price\"].round(2)\n",
    "rf_comparison[\"Predicted_Price_RF\"] = rf_comparison[\"Predicted_Price_RF\"].round(2)\n",
    "\n",
    "# Show the first 20 rows\n",
    "print(rf_comparison.head(20))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred_random_forest_regressor, alpha=0.6, edgecolors=\"k\")\n",
    "plt.xlabel(\"Actual Sale Price ($)\")\n",
    "plt.ylabel(\"Predicted Sale Price ($)\")\n",
    "plt.title(\"Random Forest — Actual vs Predicted Sale Prices\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)  # perfect prediction line\n",
    "plt.show()\n",
    "\n",
    "# Pick a small sample to keep the plot readable (change n if you want)\n",
    "sample = rf_comparison.head(20)\n",
    "\n",
    "# Plot actual vs predicted as two lines\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sample.index, sample[\"Actual_Price\"], label=\"Actual Price\", marker=\"o\")\n",
    "plt.plot(sample.index, sample[\"Predicted_Price_RF\"], label=\"Predicted Price (RF)\", marker=\"x\")\n",
    "plt.title(\"Random Forest — Actual vs Predicted Sale Prices (Sample)\")\n",
    "plt.xlabel(\"House Index\")\n",
    "plt.ylabel(\"Sale Price ($)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# keep both panels on the same scale\n",
    "xmin, xmax = y_test.min(), y_test.max()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=False)\n",
    "\n",
    "# --- Linear Regression ---\n",
    "axes[0].scatter(y_test, y_pred_linear_regression, alpha=0.6, edgecolors=\"k\")\n",
    "axes[0].plot([xmin, xmax], [xmin, xmax], \"r--\", lw=2)\n",
    "axes[0].set_xlim(xmin, xmax); axes[0].set_ylim(xmin, xmax)\n",
    "axes[0].set_title(\"Linear Regression — Actual vs Predicted\", pad=14)\n",
    "axes[0].set_xlabel(\"Actual Sale Price ($)\")\n",
    "axes[0].set_ylabel(\"Predicted Sale Price ($)\")\n",
    "\n",
    "# --- Random Forest ---\n",
    "axes[1].scatter(y_test, y_pred_random_forest_regressor, alpha=0.6, edgecolors=\"k\")\n",
    "axes[1].plot([xmin, xmax], [xmin, xmax], \"r--\", lw=2)\n",
    "axes[1].set_xlim(xmin, xmax); axes[1].set_ylim(xmin, xmax)\n",
    "axes[1].set_title(\"Random Forest — Actual vs Predicted\", pad=14)\n",
    "axes[1].set_xlabel(\"Actual Sale Price ($)\")\n",
    "axes[1].set_ylabel(\"Predicted Sale Price ($)\")\n",
    "\n",
    "# add some spacing between subplots to prevent overlap\n",
    "fig.subplots_adjust(wspace=0.35, left=0.07, right=0.98, top=0.92, bottom=0.10)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
